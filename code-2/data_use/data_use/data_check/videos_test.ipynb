{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mew_0.01',\n",
       " 'mew_noise',\n",
       " 'mew_noise_0.01',\n",
       " 'mew',\n",
       " 'mew_0.01_bis',\n",
       " 'mew_0.001_noisy',\n",
       " 'mew_0.001_normal',\n",
       " 'mew_0.01_noisy',\n",
       " 'mew_0.01_normal']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = '/scratch/users/jpierre'\n",
    "os.listdir(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mew_0.001_normal'\n",
    "torch_dataset = f'/scratch/users/jpierre/{dataset}/training/torch_file'\n",
    "np_dataset = f'/scratch/users/jpierre/{dataset}/training/np_file'\n",
    "df_file = f'/scratch/users/jpierre/{dataset}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = os.listdir(torch_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_0_step_0.pt\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_steps(path, simulation_number):\n",
    "    pattern = re.compile(f\"sim_{simulation_number}_step_(\\d+)\\.pt\")\n",
    "\n",
    "    steps = []; res = []\n",
    "\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(path):\n",
    "        # Check if the file matches the pattern\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            step = int(match.group(1))\n",
    "            steps.append(step)\n",
    "    \n",
    "    steps.sort()\n",
    "    \n",
    "    for i in range(len(steps)):\n",
    "        res.append(torch.load(os.path.join(path, f\"sim_{simulation_number}_step_{steps[i]}.pt\")))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpierre/anaconda3/envs/myenvPy/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "r = collect_steps(torch_dataset, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[300, 10], edge_index=[2, 344], edge_attr=[344, 5], y=[300, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "print(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_out_grpah(data_graph, video_params, bounds=None):\n",
    "    \"\"\"\n",
    "    Creates an MP4 video from a PyTorch tensor representing graph nodes and edges using cv2,\n",
    "    based on specified video parameters and position bounds.\n",
    "\n",
    "    Parameters:\n",
    "    - data_x: List of input features of the graph\n",
    "    - edge_indices: List of tensors for the different indices\n",
    "    - video_params: An instance of VideoParameters class containing video settings.\n",
    "    - bounds: Tuple of ((min_x, max_x), (min_y, max_y)) specifying the bounds for the positions.\n",
    "              If None, uses the minimum and maximum values from the data.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ## get back the array with teh positions\n",
    "\n",
    "    mat_pos = []\n",
    "    mat_indices = []\n",
    "\n",
    "    for i in range(len(data_graph)):\n",
    "        mat_pos.append(data_graph[i].x[:, :2])\n",
    "\n",
    "        mat_indices.append(data_graph[i].edge_index)    \n",
    "\n",
    "\n",
    "    mat_pos = np.stack(mat_pos, axis = 0)  \n",
    "    print(len(mat_indices))\n",
    "    print(mat_pos.shape)\n",
    "        \n",
    "    if bounds:\n",
    "        min_x, max_x = bounds[0]\n",
    "        min_y, max_y = bounds[1]\n",
    "    else:\n",
    "        min_x, max_x = mat_pos[:, :, 0].min(), mat_pos[:, :, 0].max()\n",
    "        min_y, max_y = mat_pos[:, :, 1].min(), mat_pos[:, :, 1].max()\n",
    "\n",
    "    # Normalize coordinates to fit within the video frame size\n",
    "    mat_pos[:, :, 0] = (mat_pos[:, :, 0] - min_x) / (max_x - min_x) * (video_params.size[0] - 1)\n",
    "    mat_pos[:, :, 1] = (mat_pos[:, :, 1] - min_y) / (max_y - min_y) * (video_params.size[1] - 1)\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Be sure to use lowercase\n",
    "    out = cv2.VideoWriter(video_params.path, fourcc, video_params.fps, video_params.size)\n",
    "    \n",
    "    mat_pos = mat_pos.astype(int)\n",
    "\n",
    "    for i in range(mat_pos.shape[0]):\n",
    "        frame = np.zeros((video_params.size[1], video_params.size[0], 3), dtype=np.uint8)\n",
    "\n",
    "        # Draw nodes\n",
    "        for x, y in mat_pos[i]:\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Draw edges\n",
    "        for start_node, end_node in mat_indices[i].t().numpy():\n",
    "            pt1 = tuple(mat_pos[i, start_node, :])\n",
    "            pt2 = tuple(mat_pos[i, end_node, :])\n",
    "            cv2.line(frame, pt1, pt2, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_VIDEO = {'radius': 5, 'color': (0, 255, 0)}\n",
    "\n",
    "\n",
    "class videoParameters():\n",
    "    def __init__(self, path:str, params = CFG_VIDEO):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "        -----\n",
    "            - `path`: path of the video\n",
    "        \"\"\"\n",
    "        self.fps = 10\n",
    "        self.path = path\n",
    "        self.size = (600, 600)\n",
    "        self.params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = videoParameters(path = os.path.join(os.getcwd(), 'test.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989\n",
      "(989, 300, 2)\n"
     ]
    }
   ],
   "source": [
    "display_out_grpah(r, p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenvPy]",
   "language": "python",
   "name": "conda-env-myenvPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

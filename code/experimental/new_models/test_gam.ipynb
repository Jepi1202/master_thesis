{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conditioner sur la dimension pour rollout probablement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_products(num_variables, degree):\n",
    "    \"\"\"\n",
    "    Compute the number of polynomial products for a given number of variables and degree.\n",
    "    \n",
    "    Args:\n",
    "        num_variables (int): The number of variables.\n",
    "        degree (int): The maximum degree of the polynomial terms.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of polynomial products.\n",
    "    \"\"\"\n",
    "    num_products = 0\n",
    "    for d in range(1, degree + 1):\n",
    "        num_products += sum(1 for _ in itertools.combinations_with_replacement(range(num_variables), d))\n",
    "    return num_products\n",
    "\n",
    "def compute_products(tensor, degree):\n",
    "    \"\"\"\n",
    "    Compute all polynomial products up to a given degree for the input tensor using PyTorch operations.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor with variables.\n",
    "        degree (int): The maximum degree of the polynomial terms.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing all polynomial products up to the given degree.\n",
    "    \"\"\"\n",
    "    num_variables = tensor.shape[0]\n",
    "    num_products = compute_num_products(num_variables, degree)\n",
    "    products = torch.empty(num_products)\n",
    "\n",
    "    idx = 0\n",
    "    for d in range(1, degree + 1):\n",
    "        for combo in itertools.combinations_with_replacement(range(num_variables), d):\n",
    "            product = torch.prod(tensor[list(combo)])\n",
    "            products[idx] = product\n",
    "            idx += 1\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  1.,  2.,  5.,  5., 16.,  4.,  8., 20., 20.,  1.,  2.,  5.,  5.,\n",
      "         4., 10., 10., 25., 25., 25.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([4, 1, 2, 5, 5])\n",
    "\n",
    "print(compute_products(a, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.,   2.,   3.,   1.,   2.,   3.,   4.,   6.,   9.,   1.,   2.,   3.,\n",
      "           4.,   6.,   9.,   8.,  12.,  18.,  27.],\n",
      "        [  4.,   5.,   6.,  16.,  20.,  24.,  25.,  30.,  36.,  64.,  80.,  96.,\n",
      "         100., 120., 144., 125., 150., 180., 216.]], grad_fn=<CopySlices>)\n",
      "tensor([  4.0998, 136.5760], grad_fn=<MvBackward0>)\n",
      "tensor([[12.1050,  6.1875, -8.6075],\n",
      "        [78.0966, 10.9784,  4.1306]])\n",
      "tensor([  5.,   7.,   9.,  17.,  22.,  27.,  29.,  36.,  45.,  65.,  82.,  99.,\n",
      "        104., 126., 153., 133., 162., 198., 243.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "\n",
    "def compute_num_products(num_variables, degree):\n",
    "    \"\"\"\n",
    "    Compute the number of polynomial products for a given number of variables and degree.\n",
    "    \n",
    "    Args:\n",
    "        num_variables (int): The number of variables.\n",
    "        degree (int): The maximum degree of the polynomial terms.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of polynomial products.\n",
    "    \"\"\"\n",
    "    num_products = 0\n",
    "    for d in range(1, degree + 1):\n",
    "        num_products += sum(1 for _ in itertools.combinations_with_replacement(range(num_variables), d))\n",
    "    return num_products\n",
    "\n",
    "def compute_products_batch(tensor, degree):\n",
    "    \"\"\"\n",
    "    Compute all polynomial products up to a given degree for a batch of input tensors using PyTorch operations.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor with shape [batch_size, num_variables].\n",
    "        degree (int): The maximum degree of the polynomial terms.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing all polynomial products for the batch, shape [batch_size, num_products].\n",
    "    \"\"\"\n",
    "    batch_size, num_variables = tensor.shape\n",
    "    num_products = compute_num_products(num_variables, degree)\n",
    "    products = torch.empty(batch_size, num_products, device=tensor.device)\n",
    "\n",
    "    idx = 0\n",
    "    for d in range(1, degree + 1):\n",
    "        for combo in itertools.combinations_with_replacement(range(num_variables), d):\n",
    "            product = torch.prod(tensor[:, list(combo)], dim=1)\n",
    "            products[:, idx] = product\n",
    "            idx += 1\n",
    "\n",
    "    return products\n",
    "\n",
    "def weighted_sum_batch(products, weights):\n",
    "    \"\"\"\n",
    "    Compute the weighted sum of the polynomial products for a batch.\n",
    "    \n",
    "    Args:\n",
    "        products (torch.Tensor): The tensor containing polynomial products, shape [batch_size, num_products].\n",
    "        weights (torch.Tensor): The corresponding weights for each polynomial product, shape [num_products].\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The weighted sum of polynomial products for the batch, shape [batch_size].\n",
    "    \"\"\"\n",
    "    return torch.matmul(products, weights)\n",
    "\n",
    "# Example usage\n",
    "variables = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True)  # Example batch of variables\n",
    "degree = 3\n",
    "products = compute_products_batch(variables, degree)\n",
    "\n",
    "print(products)\n",
    "\n",
    "num_products = products.shape[1]\n",
    "weights = torch.randn(num_products, requires_grad=True)  # Random weights for each polynomial product\n",
    "\n",
    "result = weighted_sum_batch(products, weights)\n",
    "print(result)\n",
    "\n",
    "# To check the computation graph, we can compute gradients\n",
    "result.sum().backward()  # Summing results to compute a single scalar gradient\n",
    "print(variables.grad)\n",
    "print(weights.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import itertools\n",
    "\n",
    "\n",
    "LATENT_SHAPE = 128\n",
    "\n",
    "# GNN related parameters\n",
    "EDGES_SHAPE = 5\n",
    "MESSAGE_SHAPE = 128\n",
    "HIDDEN_NN_SHAPE = 128\n",
    "\n",
    "\n",
    "# output\n",
    "OUTPUT_SHAPE= 2\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" \n",
    "    linearly growing size\n",
    "    \"\"\"\n",
    "    def __init__(self, inputShape:int, outputShape:int, dropout:float = 0.3):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.inputShape = inputShape\n",
    "        self.outputShape = outputShape\n",
    "\n",
    "        self.delta = (inputShape - outputShape) // 3\n",
    "        dim1 = inputShape - self.delta\n",
    "        dim2 = dim1 - self.delta\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(inputShape, dim1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim1, dim2),\n",
    "            #nn.ELU(),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim2, outputShape),\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def init_weights(self):\n",
    "        for layer in self.mlp.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity='leaky_relu')\n",
    "                #nn.init.xavier_normal_(layer.weight)\n",
    "                #nn.init.zeros_(layer.bias)\n",
    "                layer.bias.data.fill_(0.)\n",
    "                \n",
    " \n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    \"\"\"\n",
    "    constant size\n",
    "    \"\"\"\n",
    "    def __init__(self, inputShape:int, latentShape:int, outputShape:int, dropout:float = 0.3):\n",
    "        super(MLP2, self).__init__()\n",
    "\n",
    "        self.inputShape = inputShape\n",
    "        self.latentShape = latentShape\n",
    "        self.outputShape = outputShape\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(inputShape, latentShape),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(latentShape, latentShape),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(latentShape, outputShape),\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for layer in self.mlp.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity='leaky_relu')\n",
    "                #nn.init.xavier_normal_(layer.weight)\n",
    "                #nn.init.zeros_(layer.bias)\n",
    "                layer.bias.data.fill_(0.)\n",
    "                \n",
    "\n",
    "def compute_num_products(num_variables, degree):\n",
    "    \"\"\"\n",
    "    Compute the number of polynomial products for a given number of variables and degree.\n",
    "    \n",
    "    Args:\n",
    "        num_variables (int): The number of variables.\n",
    "        degree (int): The maximum degree of the polynomial terms.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of polynomial products.\n",
    "    \"\"\"\n",
    "    num_products = 0\n",
    "    for d in range(1, degree + 1):\n",
    "        num_products += sum(1 for _ in itertools.combinations_with_replacement(range(num_variables), d))\n",
    "    return num_products\n",
    "\n",
    "def compute_products_batch(tensor, degree):\n",
    "    \"\"\"\n",
    "    Compute all polynomial products up to a given degree for a batch of input tensors using PyTorch operations.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor with shape [batch_size, num_variables].\n",
    "        degree (int): The maximum degree of the polynomial terms.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing all polynomial products for the batch, shape [batch_size, num_products].\n",
    "    \"\"\"\n",
    "    batch_size, num_variables = tensor.shape\n",
    "    num_products = compute_num_products(num_variables, degree)\n",
    "    products = torch.empty(batch_size, num_products, device=tensor.device)\n",
    "\n",
    "    idx = 0\n",
    "    for d in range(1, degree + 1):\n",
    "        for combo in itertools.combinations_with_replacement(range(num_variables), d):\n",
    "            product = torch.prod(tensor[:, list(combo)], dim=1)\n",
    "            products[:, idx] = product\n",
    "            idx += 1\n",
    "\n",
    "    return products\n",
    "\n",
    "\n",
    "                \n",
    "class GN_edge_GAM(MessagePassing):\n",
    "    \"\"\" \n",
    "    Message passing neural network in which the message passing\n",
    "    only considers the edges features\n",
    "    \"\"\"\n",
    "    def __init__(self, inputShape:int, outputShape:int, degreePoly = 2, nbDim =2,  shapeEdges:int = 7, hiddenShape:int=64, aggr:str='add'):\n",
    "        super(GN_edge_GAM, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.inputShape = inputShape\n",
    "        self.degreePoly = degreePoly\n",
    "        self.nbDim = nbDim\n",
    "        self.messageShape = compute_num_products(shapeEdges, degreePoly) * nbDim\n",
    "        self.outputShape = outputShape\n",
    "        self.hiddenShape = hiddenShape\n",
    "        \n",
    "        self.messageMLP = MLP2(shapeEdges, hiddenShape, self.messageShape)\n",
    "        self.norm = torch.nn.LayerNorm(self.nbDim+ inputShape)\n",
    "        \n",
    "        self.updateMLP = MLP(self.nbDim + inputShape, outputShape)\n",
    "    \n",
    "    \n",
    "    def forward(self, x:torch.tensor, edge_index:torch.tensor, edge_attr: torch.tensor):\n",
    "        # Propagate messages\n",
    "        out = self.propagate(edge_index, size=(x.size(0), x.size(0)), edge_attr=edge_attr, x=x)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def message(self, x_i:torch.tensor, x_j:torch.tensor, edge_attr: torch.tensor):\n",
    "        \"\"\" \n",
    "        Perfomrs the message passing in the graph neural network\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `x_i`: tensor associated to node i\n",
    "        - `x_j`: tensor associated to node j\n",
    "        \"\"\"\n",
    "\n",
    "        # compute the products\n",
    "        prods = compute_products_batch(edge_attr, self.degreePoly)\n",
    "        shape = prods.shape\n",
    "        prods = prods.repeat(1, self.nbDim).reshape(2*shape[0], shape[1])\n",
    "        # needs to have \n",
    "        #print(prods)\n",
    "        #print(prods)\n",
    "        # repeat along some axis\n",
    "\n",
    "        # obtain the weights\n",
    "\n",
    "        weights = self.messageMLP(edge_attr).reshape(prods.shape)\n",
    "        print(weights.shape)\n",
    "\n",
    "        res = torch.sum(weights * prods, axis = -1)\n",
    "\n",
    "        print(res.shape)\n",
    "        print(res.shape[0]/self.nbDim)\n",
    "\n",
    "        res = res.reshape(int(res.shape[0]/self.nbDim), self.nbDim)\n",
    "\n",
    "        print(res.shape)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def update(self, aggr_out:torch.tensor, x:torch.tensor):\n",
    "        \"\"\" \n",
    "        Function to update all the nodes after the aggregation\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `aggr_out`: result after the aggregation [# Nodes, messageShape]\n",
    "        - `x`: current node [1, inputShape]\n",
    "        \"\"\"\n",
    "        print(aggr_out.shape)\n",
    "        xVal = self.norm(torch.cat([x, aggr_out], dim=-1))\n",
    "        return self.updateMLP(xVal) \n",
    "\n",
    " \n",
    "\n",
    "class GAM_GNN(nn.Module):\n",
    "    def __init__(self, inShape:int, latentShape:int, outShape:int, nbMessages:int, basis, edge_shape:int = EDGES_SHAPE, hiddenGN:int = 128):\n",
    "        \"\"\" \n",
    "        Neural network to combine everything\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `inShape`: shape of the input vector\n",
    "        - `latentShape`: shape of the latent space\n",
    "        - `outShape`: shape of the output vector\n",
    "        - `messageShape`: shape of the message in the GN\n",
    "        - `hiddenGN`: shape of the hidden layers in the MLP of the GN\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.inShape = inShape\n",
    "        self.latentShape = latentShape\n",
    "        self.outShape = outShape\n",
    "        \n",
    "        \n",
    "        # GNN\n",
    "        self.GNN = GN_edge_GAM(self.inShape, self.outShape, 2, 2, edge_shape, hiddenGN)\n",
    "        \n",
    "    def forward(self, graph):\n",
    "        \"\"\" \n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `x`: value for the nodes [# Nodes, #Timesteps x inShape]\n",
    "        - `edge_index`\n",
    "        - `edge_attr`\n",
    "        \"\"\"\n",
    "\n",
    "        x = graph.x\n",
    "        edge_index = graph.edge_index\n",
    "        edge_attr = graph.edge_attr\n",
    "        \n",
    "        \n",
    "        # encoder part\n",
    "        #y = self.applyEnc(x)\n",
    "        \n",
    "        # gnn part\n",
    "        y = self.GNN(x, edge_index, edge_attr)                                     # [#Nodes, outSHpae]\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def applyEnc(self, x):                                                    \n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def L1Reg(self, graph):\n",
    "        atr = graph.edge_attr\n",
    "\n",
    "        messages = self.GNN.message(None, None, atr)\n",
    "\n",
    "        loss = 0.01 * torch.sum(torch.abs(messages)) / graph.edge_index[0, :].shape[0]\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def loadNetwork(inputShape, edge_shape = EDGES_SHAPE):\n",
    "    print('>>>> loading simplest')\n",
    "    print('INFO >>> with NO encoder')\n",
    "    print('INFO >>> with NO dropout')\n",
    "    net = GAM_GNN(inputShape, LATENT_SHAPE, OUTPUT_SHAPE, MESSAGE_SHAPE,edge_shape, HIDDEN_NN_SHAPE)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = compute_products_batch(attr[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([134, 20])\n"
     ]
    }
   ],
   "source": [
    "print(basis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lim = 0.85 * 100\n",
    "\n",
    "xPos = np.linspace(-lim, lim, 10)\n",
    "yPos = np.linspace(-lim, lim, 10)\n",
    "gridX, gridY = np.meshgrid(xPos, yPos)\n",
    "delta = np.random.uniform(0, 7, gridX.shape + (2,))\n",
    "\n",
    "gridX2 = gridX + delta[:, :, 0]\n",
    "gridY2 = gridY + delta[:, :, 1]\n",
    "\n",
    "pos = np.stack([gridX.ravel(), gridY.ravel()], axis=1)\n",
    "pos_perturbed = np.stack([gridX2.ravel(), gridY2.ravel()], axis=1)\n",
    "\n",
    "pos = np.concatenate([pos, pos_perturbed], axis=0)\n",
    "\n",
    "angles = np.random.rand(pos.shape[0]) * 2 * np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def path_link(path:str):\n",
    "    sys.path.append(path)\n",
    "\n",
    "path_link('master/code/lib')\n",
    "\n",
    "import simulation_v2 as sim2\n",
    "import features as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0:60, tau:3.5, k:70, epsilon:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:10<00:00, 99.39it/s] \n"
     ]
    }
   ],
   "source": [
    "data = sim2.compute_main(200, (60, 3.5, 70, 0.5), 10, T = 1000, initialization = (pos, angles), dt = 0.001, seed = 42)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, attr, inds = ft.processSimulation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Data(x = x[0][:, 2:], y = y[0], edge_attr = attr[0], edge_index = inds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 8])\n"
     ]
    }
   ],
   "source": [
    "print(g.x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98, 5])\n"
     ]
    }
   ],
   "source": [
    "print(g.edge_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = GAM_GNN(8, 128, 2, 2, compute_products_batch, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(mod.GNN.messageShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 20])\n",
      "torch.Size([196])\n",
      "98.0\n",
      "torch.Size([98, 2])\n",
      "torch.Size([200, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1424,  0.7505],\n",
       "        [ 0.1416,  0.7423],\n",
       "        [ 0.3497,  2.2736],\n",
       "        [ 0.1413,  0.7326],\n",
       "        [ 0.2291,  1.6039],\n",
       "        [ 0.5131,  2.9059],\n",
       "        [ 0.5609,  1.8078],\n",
       "        [ 0.6948,  2.1609],\n",
       "        [ 0.3780,  1.8856],\n",
       "        [ 0.2508,  2.5541],\n",
       "        [ 0.6593,  2.0728],\n",
       "        [ 0.1407,  0.7295],\n",
       "        [ 0.1414,  0.7368],\n",
       "        [ 0.0191,  0.6627],\n",
       "        [ 0.1421,  0.7506],\n",
       "        [ 0.1188,  1.3606],\n",
       "        [ 0.1440,  0.7734],\n",
       "        [ 0.1196,  1.3624],\n",
       "        [ 0.1401,  0.7175],\n",
       "        [ 0.1389,  0.6987],\n",
       "        [ 0.6889,  2.1468],\n",
       "        [ 0.6722,  2.1057],\n",
       "        [ 0.1409,  0.7256],\n",
       "        [ 0.1393,  0.7103],\n",
       "        [ 0.1410,  0.7259],\n",
       "        [ 0.1419,  0.7467],\n",
       "        [ 0.1430,  0.7609],\n",
       "        [ 0.1420,  0.7514],\n",
       "        [ 0.1427,  0.7605],\n",
       "        [ 0.5520,  1.6455],\n",
       "        [ 0.1437,  0.7802],\n",
       "        [ 0.3345,  1.8098],\n",
       "        [ 0.1997,  2.7438],\n",
       "        [ 0.1378,  0.3816],\n",
       "        [ 0.1886,  1.4646],\n",
       "        [ 0.6457,  2.0375],\n",
       "        [-0.0452,  0.8544],\n",
       "        [-0.0451,  0.8510],\n",
       "        [ 0.1570,  0.2431],\n",
       "        [ 0.4568,  3.0834],\n",
       "        [ 0.4246,  3.0704],\n",
       "        [ 0.5292,  1.7194],\n",
       "        [ 0.2987,  1.7430],\n",
       "        [ 0.2224,  1.5899],\n",
       "        [ 0.1412,  0.7293],\n",
       "        [ 0.2359,  2.8188],\n",
       "        [ 0.1339,  0.6168],\n",
       "        [ 0.6363,  2.0127],\n",
       "        [ 0.6963,  2.1645],\n",
       "        [ 0.4947,  2.0617],\n",
       "        [ 0.1303,  0.5868],\n",
       "        [ 0.1516,  0.2945],\n",
       "        [ 0.3444,  2.9771],\n",
       "        [ 0.7149,  2.2053],\n",
       "        [ 0.1390,  0.7033],\n",
       "        [ 0.1409,  0.7273],\n",
       "        [ 0.1430,  0.7596],\n",
       "        [ 0.1256,  0.4878],\n",
       "        [ 0.1416,  0.7432],\n",
       "        [ 0.0367,  0.6646],\n",
       "        [ 0.1403,  0.7145],\n",
       "        [ 0.1436,  0.7658],\n",
       "        [ 0.2233,  2.7378],\n",
       "        [ 0.4503,  3.0841],\n",
       "        [ 0.1337,  0.6244],\n",
       "        [ 0.4297,  3.0749],\n",
       "        [ 0.5958,  1.4178],\n",
       "        [ 0.1397,  0.7133],\n",
       "        [ 0.6741,  2.1104],\n",
       "        [ 0.5544,  1.3471],\n",
       "        [-0.2253,  1.4664],\n",
       "        [ 0.1417,  0.7386],\n",
       "        [ 0.1422,  0.7530],\n",
       "        [ 0.5736,  2.1533],\n",
       "        [ 0.1421,  0.7453],\n",
       "        [ 0.1413,  0.7314],\n",
       "        [ 0.4944,  3.0229],\n",
       "        [ 0.5901,  1.5076],\n",
       "        [ 0.7079,  2.2176],\n",
       "        [ 0.7153,  2.2060],\n",
       "        [ 0.1414,  0.7391],\n",
       "        [ 0.1418,  0.7530],\n",
       "        [ 0.6757,  2.1145],\n",
       "        [ 0.1419,  0.7424],\n",
       "        [ 0.7099,  2.2170],\n",
       "        [ 0.0143,  0.2220],\n",
       "        [ 0.1402,  0.7226],\n",
       "        [-0.1413,  1.0915],\n",
       "        [ 0.5001,  2.9988],\n",
       "        [ 0.3275,  2.9549],\n",
       "        [ 0.1338,  0.6462],\n",
       "        [ 0.1394,  0.2130],\n",
       "        [ 0.2211,  1.9550],\n",
       "        [ 0.5926,  1.4837],\n",
       "        [ 0.1289,  0.4492],\n",
       "        [ 0.3115,  2.5340],\n",
       "        [ 0.3405,  2.3551],\n",
       "        [ 0.1414,  0.7316],\n",
       "        [ 0.1414,  0.7326],\n",
       "        [ 0.1391,  0.7013],\n",
       "        [ 0.1316,  0.5961],\n",
       "        [ 0.1386,  0.6930],\n",
       "        [ 0.5878,  2.1667],\n",
       "        [ 0.1355,  0.6478],\n",
       "        [ 0.6263,  1.9863],\n",
       "        [ 0.2973,  2.5936],\n",
       "        [ 0.5873,  1.8807],\n",
       "        [ 0.5395,  1.4322],\n",
       "        [ 0.3777,  3.0185],\n",
       "        [ 0.2243,  1.5941],\n",
       "        [ 0.3140,  2.5218],\n",
       "        [ 0.1357,  0.6557],\n",
       "        [ 0.1388,  0.6960],\n",
       "        [-0.0408,  0.7682],\n",
       "        [ 0.1410,  0.7326],\n",
       "        [-0.0398,  0.7534],\n",
       "        [ 0.1418,  0.7409],\n",
       "        [ 0.2469,  2.3567],\n",
       "        [ 0.1380,  0.6810],\n",
       "        [ 0.1349,  0.4214],\n",
       "        [-0.0457,  0.8685],\n",
       "        [ 0.5877,  1.5265],\n",
       "        [ 0.1353,  0.6461],\n",
       "        [ 0.1308,  0.5855],\n",
       "        [ 0.1347,  0.6409],\n",
       "        [ 0.1396,  0.7117],\n",
       "        [ 0.1402,  0.7192],\n",
       "        [ 0.1407,  0.7276],\n",
       "        [ 0.1402,  0.7159],\n",
       "        [ 0.4690,  1.7480],\n",
       "        [ 0.1424,  0.7556],\n",
       "        [ 0.5943,  1.4174],\n",
       "        [ 0.4816,  3.0578],\n",
       "        [ 0.1284,  0.5642],\n",
       "        [ 0.3131,  2.9351],\n",
       "        [ 0.5883,  1.5222],\n",
       "        [ 0.5532,  1.7866],\n",
       "        [ 0.5922,  2.1706],\n",
       "        [ 0.1291,  0.5786],\n",
       "        [-0.0117,  0.6862],\n",
       "        [ 0.3325,  1.8062],\n",
       "        [ 0.4482,  1.8294],\n",
       "        [ 0.7101,  2.1954],\n",
       "        [ 0.5017,  2.6238],\n",
       "        [ 0.1343,  0.6345],\n",
       "        [ 0.5491,  1.3265],\n",
       "        [ 0.1306,  0.5786],\n",
       "        [ 0.6694,  2.0985],\n",
       "        [ 0.1889,  1.4695],\n",
       "        [ 0.7171,  2.2096],\n",
       "        [ 0.1275,  0.5470],\n",
       "        [ 0.1392,  0.3925],\n",
       "        [ 0.5927,  1.4822],\n",
       "        [ 0.0530,  1.2031],\n",
       "        [ 0.1433,  0.3627],\n",
       "        [ 0.1346,  0.6376],\n",
       "        [ 0.1307,  0.5830],\n",
       "        [ 0.1312,  0.5893],\n",
       "        [ 0.1310,  0.5859],\n",
       "        [ 0.0900,  0.7170],\n",
       "        [ 0.1351,  0.6459],\n",
       "        [ 0.1423,  0.7488],\n",
       "        [ 0.5221,  2.6524],\n",
       "        [-0.0464,  0.8881],\n",
       "        [ 0.1452,  0.3520],\n",
       "        [ 0.5521,  1.3401],\n",
       "        [ 0.6808,  2.2193],\n",
       "        [ 0.1284,  0.5570],\n",
       "        [-0.0380,  0.7222],\n",
       "        [-0.0453,  0.8570],\n",
       "        [ 0.1770,  1.0052],\n",
       "        [ 0.1336,  0.6243],\n",
       "        [ 0.1288,  0.5604],\n",
       "        [ 0.4199,  1.9333],\n",
       "        [ 0.1386,  0.6956],\n",
       "        [ 0.1350,  0.6406],\n",
       "        [ 0.1990,  1.6207],\n",
       "        [ 0.3238,  2.9498],\n",
       "        [ 0.1806,  1.3466],\n",
       "        [ 0.3397,  2.5751],\n",
       "        [ 0.1335,  0.6199],\n",
       "        [ 0.1410,  0.7315],\n",
       "        [ 0.6411,  2.2057],\n",
       "        [ 0.1390,  0.6982],\n",
       "        [ 0.0037,  0.6750],\n",
       "        [ 0.1423,  0.3800],\n",
       "        [ 0.1308,  0.5878],\n",
       "        [-0.0155,  0.3712],\n",
       "        [ 0.5170,  2.8560],\n",
       "        [ 0.5142,  2.8937],\n",
       "        [ 0.1395,  0.7130],\n",
       "        [ 0.1541,  0.2860],\n",
       "        [ 0.3061,  1.7572],\n",
       "        [ 0.1945,  2.7431],\n",
       "        [ 0.1331,  0.6032],\n",
       "        [ 0.5302,  1.7223],\n",
       "        [ 0.4243,  3.0701],\n",
       "        [ 0.1282,  0.4646],\n",
       "        [ 0.1382,  0.6867],\n",
       "        [ 0.1353,  0.6486]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import itertools\n",
    "\n",
    "\n",
    "GAM_CFG = {\n",
    "    \"input_shape\": 8,\n",
    "    \"edges_shape\": 5,\n",
    "    \"output_shape\": 2,\n",
    "    \"MLP_message\": {\n",
    "        \"hidden_shape\": 128,\n",
    "        \"dropout\": \"no\"\n",
    "    },\n",
    "    \"MLP_update\": {\n",
    "        \"hidden_shape\": 128,\n",
    "        \"dropout\": \"no\"\n",
    "    },\n",
    "    \"Basis\": {\n",
    "        \"basis\": \"poly\",\n",
    "        \"degree\": 2,\n",
    "        \"nDim\": 2,\n",
    "    },\n",
    "    \"regularization\": {\n",
    "        \"name\": \"l1\",\n",
    "        \"scaler\": 0.001\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def compute_num_products(num_variables, degree):\n",
    "    \"\"\"\n",
    "    Compute the number of polynomial products for a given number of variables and degree.\n",
    "    \n",
    "    Args:\n",
    "        num_variables (int): The number of variables.\n",
    "        degree (int): The maximum degree of the polynomial terms.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of polynomial products.\n",
    "    \"\"\"\n",
    "    num_products = 0\n",
    "    for d in range(1, degree + 1):\n",
    "        num_products += sum(1 for _ in itertools.combinations_with_replacement(range(num_variables), d))\n",
    "    return num_products\n",
    "\n",
    "\n",
    "def compute_products_batch(tensor, degree):\n",
    "    \"\"\"\n",
    "    Compute all polynomial products up to a given degree for a batch of input tensors using PyTorch operations.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor with shape [batch_size, num_variables].\n",
    "        degree (int): The maximum degree of the polynomial terms.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing all polynomial products for the batch, shape [batch_size, num_products].\n",
    "    \"\"\"\n",
    "    batch_size, num_variables = tensor.shape\n",
    "    num_products = compute_num_products(num_variables, degree)\n",
    "    products = torch.empty(batch_size, num_products, device=tensor.device)\n",
    "\n",
    "    idx = 0\n",
    "    for d in range(1, degree + 1):\n",
    "        for combo in itertools.combinations_with_replacement(range(num_variables), d):\n",
    "            product = torch.prod(tensor[:, list(combo)], dim=1)\n",
    "            products[:, idx] = product\n",
    "            idx += 1\n",
    "\n",
    "    return products\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" \n",
    "    linearly growing size\n",
    "    \"\"\"\n",
    "    def __init__(self, inputShape:int, outputShape:int, dropout:float = 0.3):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        print(dropout)\n",
    "        assert dropout == 'no'\n",
    "\n",
    "        self.inputShape = inputShape\n",
    "        self.outputShape = outputShape\n",
    "\n",
    "        self.delta = (inputShape - outputShape) // 3\n",
    "        dim1 = inputShape - self.delta\n",
    "        dim2 = dim1 - self.delta\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(inputShape, dim1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim1, dim2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim2, outputShape),\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def init_weights(self):\n",
    "        for layer in self.mlp.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity='leaky_relu')\n",
    "                layer.bias.data.fill_(0.)\n",
    "                \n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    \"\"\"\n",
    "    constant size\n",
    "    \"\"\"\n",
    "    def __init__(self, inputShape:int, latentShape:int, outputShape:int, dropout:float = 0.3):\n",
    "        super(MLP2, self).__init__()\n",
    "\n",
    "        self.inputShape = inputShape\n",
    "        self.latentShape = latentShape\n",
    "        self.outputShape = outputShape\n",
    "\n",
    "        print(dropout)\n",
    "        assert dropout == 'no'\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(inputShape, latentShape),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(latentShape, latentShape),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(latentShape, outputShape),\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for layer in self.mlp.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity='leaky_relu')\n",
    "                layer.bias.data.fill_(0.)\n",
    "                              \n",
    "                \n",
    "class GN_edge_GAM(MessagePassing):\n",
    "    \"\"\" \n",
    "    Message passing neural network in which the message passing\n",
    "    only considers the edges features\n",
    "    \"\"\"\n",
    "    def __init__(self, d, aggr:str='add'):\n",
    "        \n",
    "        super(GN_edge_GAM, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.inputShape = d['input_shape']\n",
    "        self.edgeShape = d['edges_shape']\n",
    "        self.outputShape = d['output_shape']\n",
    "        \n",
    "        self.degreePoly = d['Basis']['degree']\n",
    "        self.basis = d['Basis']['basis']\n",
    "        self.nbDim = d['Basis']['nDim']\n",
    "        self.messageShape = compute_num_products(d['edges_shape'], d['Basis']['degree']) * d['Basis']['nDim']\n",
    "        \n",
    "        self.hiddenShape = d['MLP_message']['hidden_shape']\n",
    "        \n",
    "        self.messageMLP = MLP2(inputShape = self.edgeShape, \n",
    "                               latentShape = self.hiddenShape, \n",
    "                               outputShape = self.messageShape, \n",
    "                               dropout = d['MLP_message']['dropout'])\n",
    "        \n",
    "        self.norm = torch.nn.LayerNorm(self.nbDim + self.inputShape)\n",
    "        \n",
    "        self.updateMLP = MLP(inputShape = self.nbDim + self.inputShape, \n",
    "                             outputShape = self.outputShape,\n",
    "                             dropout = d['MLP_update']['dropout'])\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x:torch.tensor, edge_index:torch.tensor, edge_attr: torch.tensor):\n",
    "\n",
    "        out = self.propagate(edge_index, size=(x.size(0), x.size(0)), edge_attr=edge_attr, x=x)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def message(self, x_i:torch.tensor, x_j:torch.tensor, edge_attr: torch.tensor):\n",
    "        \"\"\" \n",
    "        Perfomrs the message passing in the graph neural network\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `x_i`: tensor associated to node i\n",
    "        - `x_j`: tensor associated to node j\n",
    "        \"\"\"\n",
    "\n",
    "        # compute the products\n",
    "        prods = compute_products_batch(edge_attr, self.degreePoly)\n",
    "\n",
    "        shape = prods.shape\n",
    "        prods = prods.repeat(1, self.nbDim).reshape(self.nbDim * shape[0], shape[1])\n",
    "\n",
    "        # obtain the weights\n",
    "        weights = self.messageMLP(edge_attr).reshape(prods.shape)\n",
    "\n",
    "        res = torch.sum(weights * prods, axis = -1)\n",
    "\n",
    "        res = res.reshape(int(res.shape[0]/self.nbDim), self.nbDim)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def update(self, aggr_out:torch.tensor, x:torch.tensor):\n",
    "        \"\"\" \n",
    "        Function to update all the nodes after the aggregation\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `aggr_out`: result after the aggregation [# Nodes, messageShape]\n",
    "        - `x`: current node [1, inputShape]\n",
    "        \"\"\"\n",
    "\n",
    "        xVal = self.norm(torch.cat([x, aggr_out], dim=-1))\n",
    "        return self.updateMLP(xVal) \n",
    "\n",
    " \n",
    "class GAM_GNN(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        \"\"\" \n",
    "        Neural network to combine everything\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `inShape`: shape of the input vector\n",
    "        - `latentShape`: shape of the latent space\n",
    "        - `outShape`: shape of the output vector\n",
    "        - `messageShape`: shape of the message in the GN\n",
    "        - `hiddenGN`: shape of the hidden layers in the MLP of the GN\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.inShape = d['input_shape']\n",
    "        self.outShape = d['output_shape']\n",
    "\n",
    "        self.regu = d['regularization']['name']\n",
    "        self.scaler_regu = d['regularization']['scaler']\n",
    "        \n",
    "        \n",
    "        # GNN\n",
    "        self.GNN = GN_edge_GAM(d)\n",
    "        \n",
    "    def forward(self, graph):\n",
    "        \"\"\" \n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `x`: value for the nodes [# Nodes, #Timesteps x inShape]\n",
    "        - `edge_index`\n",
    "        - `edge_attr`\n",
    "        \"\"\"\n",
    "\n",
    "        x = graph.x\n",
    "        edge_index = graph.edge_index\n",
    "        edge_attr = graph.edge_attr\n",
    "        \n",
    "        \n",
    "        # encoder part\n",
    "        #y = self.applyEnc(x)\n",
    "        \n",
    "        # gnn part\n",
    "        y = self.GNN(x, edge_index, edge_attr)                                     # [#Nodes, outSHpae]\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def applyEnc(self, x):                                                    \n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def L1Reg(self, graph):\n",
    "        atr = graph.edge_attr\n",
    "\n",
    "        messages = self.GNN.message(None, None, atr)\n",
    "\n",
    "        loss = self.scaler_regu * torch.sum(torch.abs(messages)) / graph.edge_index[0, :].shape[0]\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def loadNetwork(d = None):\n",
    "    if d is None:\n",
    "        d = GAM_CFG\n",
    "    print('>>>> loading simplest')\n",
    "    print('INFO >>> with NO encoder')\n",
    "    print('INFO >>> with NO dropout')\n",
    "    net = GAM_GNN(d)\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import itertools\n",
    "\n",
    "\n",
    "GAM_CFG = {\n",
    "    \"input_shape\": 8,\n",
    "    \"edges_shape\": 5,\n",
    "    \"output_shape\": 2,\n",
    "    \"MLP_message\": {\n",
    "        \"hidden_shape\": 128,\n",
    "        \"dropout\": \"no\"\n",
    "    },\n",
    "    \"MLP_update\": {\n",
    "        \"hidden_shape\": 128,\n",
    "        \"dropout\": \"no\"\n",
    "    },\n",
    "    \"Basis\": {\n",
    "        \"basis\": \"poly\",\n",
    "        \"degree\": 2,\n",
    "        \"nDim\": 2,\n",
    "    },\n",
    "    \"regularization\": {\n",
    "        \"name\": \"l1\",\n",
    "        \"scaler\": 0.001\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def compute_num_products(num_variables, degree):\n",
    "    \"\"\"\n",
    "    Compute the number of polynomial products for a given number of variables and degree.\n",
    "    \n",
    "    Args:\n",
    "        num_variables (int): The number of variables.\n",
    "        degree (int): The maximum degree of the polynomial terms.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of polynomial products.\n",
    "    \"\"\"\n",
    "    num_products = 0\n",
    "    for d in range(1, degree + 1):\n",
    "        num_products += sum(1 for _ in itertools.combinations_with_replacement(range(num_variables), d))\n",
    "    return num_products\n",
    "\n",
    "\n",
    "def compute_products_batch(tensor, degree):\n",
    "    \"\"\"\n",
    "    Compute all polynomial products up to a given degree for a batch of input tensors using PyTorch operations.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor with shape [batch_size, num_variables].\n",
    "        degree (int): The maximum degree of the polynomial terms.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing all polynomial products for the batch, shape [batch_size, num_products].\n",
    "    \"\"\"\n",
    "    batch_size, num_variables = tensor.shape\n",
    "    num_products = compute_num_products(num_variables, degree)\n",
    "    products = torch.empty(batch_size, num_products, device=tensor.device)\n",
    "\n",
    "    idx = 0\n",
    "    for d in range(1, degree + 1):\n",
    "        for combo in itertools.combinations_with_replacement(range(num_variables), d):\n",
    "            product = torch.prod(tensor[:, list(combo)], dim=1)\n",
    "            products[:, idx] = product\n",
    "            idx += 1\n",
    "\n",
    "    return products\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" \n",
    "    linearly growing size\n",
    "    \"\"\"\n",
    "    def __init__(self, inputShape:int, outputShape:int, dropout:float = 0.3):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        assert dropout == 'no'\n",
    "\n",
    "        self.inputShape = inputShape\n",
    "        self.outputShape = outputShape\n",
    "\n",
    "        self.delta = (inputShape - outputShape) // 3\n",
    "        dim1 = inputShape - self.delta\n",
    "        dim2 = dim1 - self.delta\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(inputShape, dim1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim1, dim2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(dim2, outputShape),\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def init_weights(self):\n",
    "        for layer in self.mlp.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity='leaky_relu')\n",
    "                layer.bias.data.fill_(0.)\n",
    "                \n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    \"\"\"\n",
    "    constant size\n",
    "    \"\"\"\n",
    "    def __init__(self, inputShape:int, latentShape:int, outputShape:int, dropout:float = 0.3):\n",
    "        super(MLP2, self).__init__()\n",
    "\n",
    "        self.inputShape = inputShape\n",
    "        self.latentShape = latentShape\n",
    "        self.outputShape = outputShape\n",
    "\n",
    "        assert dropout == 'no'\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(inputShape, latentShape),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(latentShape, latentShape),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(latentShape, outputShape),\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for layer in self.mlp.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity='leaky_relu')\n",
    "                layer.bias.data.fill_(0.)\n",
    "                              \n",
    "                \n",
    "class GN_edge_GAM(MessagePassing):\n",
    "    \"\"\" \n",
    "    Message passing neural network in which the message passing\n",
    "    only considers the edges features\n",
    "    \"\"\"\n",
    "    def __init__(self, d, aggr:str='add'):\n",
    "        \n",
    "        super(GN_edge_GAM, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.inputShape = d['input_shape']\n",
    "        self.edgeShape = d['edges_shape']\n",
    "        self.outputShape = d['output_shape']\n",
    "        \n",
    "        self.degreePoly = d['Basis']['degree']\n",
    "        self.basis = d['Basis']['basis']\n",
    "        self.nbDim = d['Basis']['nDim']\n",
    "        self.messageShape = compute_num_products(d['edges_shape'], d['Basis']['degree']) * d['Basis']['nDim']\n",
    "        \n",
    "        self.hiddenShape = d['MLP_message']['hidden_shape']\n",
    "        \n",
    "        self.messageMLP = MLP2(inputShape = self.edgeShape, \n",
    "                               latentShape = self.hiddenShape, \n",
    "                               outputShape = self.messageShape, \n",
    "                               dropout = d['MLP_message']['dropout'])\n",
    "        \n",
    "        self.norm = torch.nn.LayerNorm(self.nbDim + self.inputShape)\n",
    "        \n",
    "        self.updateMLP = MLP(inputShape = self.nbDim + self.inputShape, \n",
    "                             outputShape = self.outputShape,\n",
    "                             dropout = d['MLP_update']['dropout'])\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x:torch.tensor, edge_index:torch.tensor, edge_attr: torch.tensor):\n",
    "\n",
    "        out = self.propagate(edge_index, size=(x.size(0), x.size(0)), edge_attr=edge_attr, x=x)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def message(self, x_i:torch.tensor, x_j:torch.tensor, edge_attr: torch.tensor):\n",
    "        \"\"\" \n",
    "        Perfomrs the message passing in the graph neural network\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `x_i`: tensor associated to node i\n",
    "        - `x_j`: tensor associated to node j\n",
    "        \"\"\"\n",
    "\n",
    "        # compute the products\n",
    "        prods = compute_products_batch(edge_attr, self.degreePoly)\n",
    "        shape = prods.shape\n",
    "        prods = prods.repeat(1, self.nbDim).reshape(self.nbDim * shape[0], shape[1])\n",
    "\n",
    "        # obtain the weights\n",
    "        weights = self.messageMLP(edge_attr).reshape(prods.shape)\n",
    "        print(weights.shape)\n",
    "\n",
    "        res = torch.sum(weights * prods, axis = -1)\n",
    "\n",
    "        res = res.reshape(int(res.shape[0]/self.nbDim), self.nbDim)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def update(self, aggr_out:torch.tensor, x:torch.tensor):\n",
    "        \"\"\" \n",
    "        Function to update all the nodes after the aggregation\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `aggr_out`: result after the aggregation [# Nodes, messageShape]\n",
    "        - `x`: current node [1, inputShape]\n",
    "        \"\"\"\n",
    "\n",
    "        xVal = self.norm(torch.cat([x, aggr_out], dim=-1))\n",
    "        return self.updateMLP(xVal) \n",
    "\n",
    " \n",
    "class GAM_GNN(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        \"\"\" \n",
    "        Neural network to combine everything\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `inShape`: shape of the input vector\n",
    "        - `latentShape`: shape of the latent space\n",
    "        - `outShape`: shape of the output vector\n",
    "        - `messageShape`: shape of the message in the GN\n",
    "        - `hiddenGN`: shape of the hidden layers in the MLP of the GN\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.inShape = d['input_shape']\n",
    "        self.outShape = d['output_shape']\n",
    "\n",
    "        self.regu = d['regularization']['name']\n",
    "        self.scaler_regu = d['regularization']['scaler']\n",
    "        \n",
    "        \n",
    "        # GNN\n",
    "        self.GNN = GN_edge_GAM(d)\n",
    "        \n",
    "    def forward(self, graph):\n",
    "        \"\"\" \n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        - `x`: value for the nodes [# Nodes, #Timesteps x inShape]\n",
    "        - `edge_index`\n",
    "        - `edge_attr`\n",
    "        \"\"\"\n",
    "\n",
    "        x = graph.x\n",
    "        edge_index = graph.edge_index\n",
    "        edge_attr = graph.edge_attr\n",
    "        \n",
    "        \n",
    "        # encoder part\n",
    "        #y = self.applyEnc(x)\n",
    "        \n",
    "        # gnn part\n",
    "        y = self.GNN(x, edge_index, edge_attr)                                     # [#Nodes, outSHpae]\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def applyEnc(self, x):                                                    \n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def L1Reg(self, graph):\n",
    "        atr = graph.edge_attr\n",
    "\n",
    "        messages = self.GNN.message(None, None, atr)\n",
    "\n",
    "        loss = self.scaler_regu * torch.sum(torch.abs(messages)) / graph.edge_index[0, :].shape[0]\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def loadNetwork(d = None):\n",
    "    if d is None:\n",
    "        d = GAM_CFG\n",
    "    print('>>>> loading simplest')\n",
    "    print('INFO >>> with NO encoder')\n",
    "    print('INFO >>> with NO dropout')\n",
    "    net = GAM_GNN(d)\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yessss\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def path_link(path:str):\n",
    "    sys.path.append(path)\n",
    "\n",
    "path_link('/master/code/lib')\n",
    "\n",
    "import utils.testing_gen as gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0:60, tau:3.5, k:70, epsilon:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 96.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0:60, tau:3.5, k:70, epsilon:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 111.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0:60, tau:3.5, k:70, epsilon:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 109.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0:60, tau:3.5, k:70, epsilon:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 80.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0:60, tau:3.5, k:70, epsilon:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 86.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0:60, tau:3.5, k:70, epsilon:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 101.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0:60, tau:3.5, k:70, epsilon:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 78.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0:60, tau:3.5, k:70, epsilon:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 93.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0:60, tau:3.5, k:70, epsilon:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 103.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0:60, tau:3.5, k:70, epsilon:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 107.32it/s]\n"
     ]
    }
   ],
   "source": [
    "NB_SIM = 10\n",
    "\n",
    "params = gen.Parameters_Simulation()\n",
    "data = gen.get_mult_data(params, NB_SIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = gen.sims2Graphs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 150, 200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> loading simplest\n",
      "INFO >>> with NO encoder\n",
      "INFO >>> with NO dropout\n"
     ]
    }
   ],
   "source": [
    "nn = loadNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228, 20])\n"
     ]
    }
   ],
   "source": [
    "a = nn(graphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
